import re
from functools import lru_cache

import socket
import urllib
import codecs
import pathlib
import base64
import binascii
from enum import Enum
from packaging.version import Version, InvalidVersion
from typing import Any, Dict, Set

from common.files import read_json_from_file


class SpecialCases(Enum):
    STRING_SENSITIVE_FILE_PATH = "STRING_SENSITIVE_FILE_PATH"
    STRING_URL = "STRING_URL"
    STRING_CONTAINS_URL = "STRING_CONTAINS_URL"
    STRING_VERSION = "STRING_VERSION"
    STRING_ENCODING = "STRING_ENCODING"
    STRING_LOCALHOST = "STRING_LOCALHOST"
    STRING_FILE_PATH = "STRING_FILE_PATH"
    STRING_IP = "STRING_IP"
    STRING_BASE64 = "STRING_BASE64"
    STRING_HEX = "STRING_HEX"
    STRING_BASH = "STRING_BASH"
    STRING_SQL = "STRING_SQL"
    STRING_CODE = "STRING_CODE"
    STRING = "STRING"
    MALFORMED_FILE = "MALFORMED_FILE"
    MALFORMED_SYNTAX = "MALFORMED_SYNTAX"
    TARGETED_FILE = "TARGETED_FILE"
    BOOLEAN = "BOOLEAN"
    INTEGER = "INTEGER"
    FLOAT = "FLOAT"
    OBJECT = "OBJECT"


SCRIPT_DIR = pathlib.Path(__file__).resolve().parent
SENSITIVE_PATHS: Set[str] = read_json_from_file(
    SCRIPT_DIR / "syntax_mapping" / "sensitive_files.json"
)
FUNCTION_MAPPING: Dict[str, Any] = read_json_from_file(
    SCRIPT_DIR / "syntax_mapping" / "function_mapping.json"
)
IMPORT_MAPPING: Dict[str, Any] = read_json_from_file(
    SCRIPT_DIR / "syntax_mapping" / "import_mapping.json"
)
COMMON_TARGET_FILES: Dict[str, Set[str]] = read_json_from_file(
    SCRIPT_DIR / "syntax_mapping" / "target_files.json"
)

# Fast, lightweight patterns for scalable code detection
# Priority: Speed over perfect accuracy for millions of files

# Pre-filters - require code-like patterns but not too restrictive
# Look for common syntactical elements that indicate code
# fmt: off
_CODE_SYNTACTICAL_ELEMENTS = frozenset({
    # Basic syntax symbols
    '(', ')', '{', '}', '[', ']', '=>', '->', '==', '!=', '&&', '||', ':', ';',
    # Programming keywords
    'return', 'import', 'function', 'def', 'class', 'print(', 'console.',
    'if ', 'for ', 'while ', 'else', 'break', 'continue',
    # HTML/XML tags
    '<html', '<body', '<div', '<script', '</',
    # Comments
    '/*', '*/',
    # Variable types
    'int ', 'var ',
})
# fmt: on

# Data exfiltration focused bash patterns for malware detection
# fmt: off
_BASH_SYNTACTICAL_ELEMENTS = frozenset({
    # Core bash syntax
    '#!/bin/', '$(' ,'${', '&&', '||', '2>&1', '|', '>>', '> ',
    'if [', 'then', 'fi', 'do', 'done', 'bash', '.sh', '$PATH',
    # Network exfiltration (primary threat)
    'curl ', 'wget ', 'nc ', 'netcat', 'ssh ', 'scp ', 'rsync ',
    # File operations for data theft  
    'cp ', 'mv ', 'rm ', 'find ', 'locate ',
    # Archive operations (compress stolen data)
    'tar ', 'gzip ', 'zip ', 'unzip ',
    # Text extraction from files
    'grep ', 'sed ', 'awk ',
    # Data encoding for obfuscation
    'base64', 'openssl',
    # Basic commands (keep for test compatibility)
    'echo ', 'ls ', 'cd ', 'cat ', 'chmod', 'sudo', 'export ',
})
# fmt: on

# SQL patterns - focus on what malware would use
# fmt: off
_SQL_SYNTACTICAL_PATTERNS = frozenset({
    # Data extraction (most common in malware)
    'SELECT ', 'FROM ', 'WHERE ',
    # Data manipulation
    'INSERT INTO', 'UPDATE ', 'SET ', 'DELETE FROM',
    # Destructive operations
    'DROP TABLE', 'DROP DATABASE', 'TRUNCATE TABLE',
    # Query modifiers and injection patterns
    'ORDER BY', 'UNION SELECT',
})
# fmt: on

# Code detection - balanced patterns to avoid false positives while catching real code
# Requires actual code structure, not just keywords
CODE_PATTERN = re.compile(
    r"("
    # Python/JS function definitions
    r"\bdef\s+\w+|\bfunction\s+\w+|\bclass\s+\w+|"
    # Lambda/arrow functions
    r"\blambda\s+.*:|\w+\s*=>|"
    # Control flow
    r"\bif\s+|\bfor\s+|\bwhile\s+|\breturn\s+|\bbreak\b|\bcontinue\b|"
    # Import statements
    r"\bimport\s+|\bfrom\s+.*\s+import|\brequire\s*\(|"
    # Variable declarations (JavaScript and C-style)
    r"\b(const|let|var|int|float|double|char|void)\s+\w+|\b(public|private)\s+|"
    # Function calls
    r"\bconsole\.\w+|\bprint\s*\(|"
    # Programming operators and C-style comments
    r"==|!=|&&|\|\||/\*[\s\S]*?\*/|//.*$|"
    # HTML/XML tags
    r"<(html|body|head|div|script|style|\?xml)[^>]*>|"
    # JSON/Object literals
    r'\{\s*["\w]+\s*:|\[\s*[\{"\d]|'
    # CSS selectors with braces
    r"\.[\w-]+\s*\{|#[\w-]+\s*\{|\w+\s*\{.*:\s*[\w#]"
    r")",
    re.IGNORECASE | re.MULTILINE,
)

# Bash detection - balanced patterns for actual bash syntax
BASH_PATTERN = re.compile(
    r"("
    # Shebang lines
    r"^#!/(usr/)?bin/(bash|sh|zsh)|"
    # Data exfiltration and theft commands
    r"\b(curl|wget|nc|netcat|ssh|scp|rsync|cp|mv|rm|find|locate|tar|gzip|zip|unzip|grep|sed|awk|base64|openssl|echo|ls|cd|cat|chmod|sudo|export)\s+|"
    # Variable expansions including PATH
    r"\$\{\w+|\$\(|\$PATH|\$HOME|\$USER|\$\w+|"
    # Command chaining and pipes
    r"&&|\|\||\||"
    # Bash conditionals
    r"if\s+\[|\];\s*then|\bdo\b|\bdone\b|\bfi\b"
    r")",
    re.IGNORECASE | re.MULTILINE,
)

# SQL detection - focus on SQL that malware would actually use
# Malware typically extracts data or drops tables, not complex DDL
SQL_PATTERN = re.compile(
    r"\b("
    # Data extraction (most common in malware)
    r"SELECT\s+.*\s+FROM\s+|"
    # Data modification
    r"INSERT\s+INTO\s+|"
    r"UPDATE\s+\w+\s+SET\s+|"
    r"DELETE\s+FROM\s+|"
    # Destructive operations
    r"DROP\s+(TABLE|DATABASE)\s+|"
    r"TRUNCATE\s+TABLE\s+|"
    # Common SQL clauses
    r"WHERE\s+\w+\s*=|"
    r"ORDER\s+BY\s+|"
    r"UNION\s+SELECT"  # SQL injection pattern
    r")\b",
    re.IGNORECASE,
)


def is_valid_ip(content: str) -> bool:
    if not content or "%" in content:
        return False
    try:
        socket.inet_pton(socket.AF_INET, content)
        return True
    except (socket.error, OSError):
        try:
            socket.inet_pton(socket.AF_INET6, content)
            return True
        except (socket.error, OSError):
            return False
    except Exception:
        return False


def is_version(version_string):
    """
    Checks if a given string is a valid version string according to PEP 440.

    Args:
        version_string (str): The string to check.

    Returns:
        bool: True if the string is a valid version, False otherwise.
    """
    try:
        # Attempt to parse the string as a Version object.
        # If it's not a valid version, InvalidVersion will be raised.
        Version(version_string)
        return True
    except InvalidVersion:
        return False


def is_valid_encoding_name(encoding_name):
    """
    Checks if a given string is a recognized Python encoding name.

    Args:
        encoding_name (str): The string to check (e.g., 'utf-8', 'latin-1').

    Returns:
        bool: True if the encoding name is recognized, False otherwise.
    """
    try:
        # Attempt to get an encoder for the given name.
        # If the name is not recognized, a LookupError will be raised.
        codecs.lookup(encoding_name)
        return True
    except LookupError:
        return False


def is_valid_url(content: str) -> bool:
    if not content or (":" not in content and "." not in content):
        return False

    if not re.match(r"^[a-zA-Z][a-zA-Z0-9+.-]*://", content):
        if content.startswith(("www.", "http.", "https.")):
            content_with_scheme = "http://" + content
        else:
            return False
    else:
        content_with_scheme = content

    try:
        result = urllib.parse.urlparse(content_with_scheme)
        return bool(result.scheme and result.netloc)
    except Exception:
        return False


def is_escaped_hex(s: str) -> bool:
    pattern = re.compile(r"^(?:\\x[0-9a-fA-F]{2})+$")
    return bool(pattern.match(s))


def is_base64(s: str) -> bool:
    base64_char_pattern = re.compile(r"^[A-Za-z0-9+/]*(={0,2})$")
    if not s:
        return False
    return bool(base64_char_pattern.match(s)) and len(s) % 4 == 0


def is_hex(text: str) -> bool:
    """
    Checks if a given string is a valid hexadecimal value.

    A string is considered hexadecimal if it optionally starts with '0x',
    has an even number of characters, and contains only valid hex digits
    (0-9 and A-F, case-insensitive).

    Args:
        text (str): The input string to check.

    Returns:
        bool: True if the string is a valid hex value, False otherwise.
    """
    if not isinstance(text, str) or not text.strip():
        return False

    # Handle the optional '0x' prefix
    if text.lower().startswith("0x"):
        text = text[2:]

    # After stripping the prefix, the string can't be empty.
    if not text:
        return False

    # A hex string representing bytes must have an even number of digits.
    if len(text) % 2 != 0:
        return False

    # The most robust way to check for hex characters is to try conversion.
    try:
        # Convert the string to an integer from base 16.
        int(text, 16)
        return True
    except ValueError:
        # The conversion failed, so it's not a valid hex string.
        return False


def contains_url(text: str) -> bool:
    """Check if a string contains a URL pattern."""
    if not text or len(text) < 8:  # Minimum URL length
        return False

    # Simple pattern to detect URLs within text
    url_patterns = [
        "http://",
        "https://",
        "ftp://",
        "ftps://",
        "ssh://",
        "telnet://",
        "file://",
        "data:",
        "javascript:",
        "vbscript:",
    ]

    for pattern in url_patterns:
        if pattern in text.lower():
            return True

    return False


def is_localhost(text: str) -> bool:
    """Check if a string represents localhost or local network addresses."""
    if not text:
        return False

    text_lower = text.lower().strip()

    # Direct localhost patterns
    localhost_patterns = [
        "localhost",
        "127.0.0.1",
        "::1",
        "0.0.0.0",
        "local",
        "loopback",
    ]

    # Check if text is exactly one of these patterns
    if text_lower in localhost_patterns:
        return True

    # Check for localhost with port
    if text_lower.startswith("localhost:") or text_lower.startswith("127.0.0.1:"):
        return True

    # Check for localhost in URLs
    if ("localhost" in text_lower or "127.0.0.1" in text_lower) and (
        "http" in text_lower or "ftp" in text_lower
    ):
        return True

    # Check for private network ranges (RFC 1918)
    import re

    # Match 192.168.x.x, 10.x.x.x, 172.16-31.x.x
    private_ip_patterns = [
        r"^192\.168\.\d{1,3}\.\d{1,3}",
        r"^10\.\d{1,3}\.\d{1,3}\.\d{1,3}",
        r"^172\.(1[6-9]|2[0-9]|3[0-1])\.\d{1,3}\.\d{1,3}",
    ]

    for pattern in private_ip_patterns:
        if re.match(pattern, text_lower):
            return True

    return False


def is_file_path(text: str) -> bool:
    if not text or len(text) < 2:
        return False

    # Exclude URLs and other non-file patterns first
    is_common_non_file_url = text.startswith(
        (
            "http://",
            "https://",
            "ftp://",
            "sftp://",
            "ws://",
            "wss://",
            "mailto:",
            "tel:",
            "data:",
        )
    )
    if is_common_non_file_url:
        return False

    # Check for common file extensions
    common_extensions = (
        ".py",
        ".js",
        ".txt",
        ".json",
        ".xml",
        ".html",
        ".css",
        ".java",
        ".cpp",
        ".c",
        ".h",
        ".hpp",
        ".sh",
        ".bat",
        ".exe",
        ".dll",
        ".so",
        ".dylib",
        ".zip",
        ".tar",
        ".gz",
        ".pdf",
        ".doc",
        ".docx",
        ".xls",
        ".xlsx",
        ".csv",
        ".log",
        ".conf",
        ".cfg",
        ".ini",
        ".yaml",
        ".yml",
        ".md",
        ".rst",
        ".tex",
        ".tmp",
        ".bak",
    )
    has_file_extension = any(text.lower().endswith(ext) for ext in common_extensions)

    # Specific file path patterns
    is_unix_like_start = text.startswith(("/", "~", "./", "../"))
    is_win_drive_start = (
        len(text) > 2
        and text[1] == ":"
        and text[0].isalpha()
        and text[2] in ("\\", "/")
    )
    is_win_unc_start = text.startswith("\\\\")

    # Must either:
    # 1. Start with a clear path pattern (absolute or relative)
    # 2. Have a file extension
    # 3. Match specific path patterns (but not generic strings with slashes)
    if is_unix_like_start or is_win_drive_start or is_win_unc_start:
        return True

    # For other cases, require both a separator and a file extension
    # or a very specific path pattern
    has_separator = "/" in text or "\\" in text
    if has_separator and has_file_extension:
        return True

    # Simple filename with extension (no path separators)
    if not has_separator and has_file_extension and "." in text:
        # Make sure it's not something like "example.com"
        # by checking it doesn't look like a domain
        if not (
            text.count(".") == 1
            and text.split(".")[1]
            in ("com", "org", "net", "edu", "gov", "io", "co", "uk")
        ):
            return True

    # Check for specific path patterns without extensions
    # (like bin paths or specific directories)
    specific_patterns = (
        "/bin/",
        "/usr/",
        "/etc/",
        "/var/",
        "/tmp/",
        "/home/",
        "/opt/",
        "/dev/",
        "/proc/",
        "/sys/",
        "C:\\Windows",
        "C:\\Program",
        "/Applications/",
        "/Library/",
        "/System/",
    )
    if any(pattern in text for pattern in specific_patterns):
        return True

    return False


def remove_newlines(text: str) -> str:
    return text.replace("\n", "").replace("\r", "")


def reduce_whitespace(text: str) -> str:
    return re.sub(r"\s+", " ", text).strip()


def clean_string_literal(url_string):
    """
    Cleans a URL string by removing various forms of quotation marks and
    the 'f' prefix from f-string literals, ensuring matching quotes.

    Args:
        url_string (str): The input string potentially containing a URL.

    Returns:
        str: The cleaned URL string.
    """

    # --- Handle f-string literal representations ---
    # Case: f"..."
    if url_string.startswith('f"') and url_string.endswith('"'):
        # Remove 'f' and the outer double quotes
        return url_string[2:-1]  # Slice off 'f"' and '"'
    # Case: f'...'
    elif url_string.startswith("f'") and url_string.endswith("'"):
        # Remove 'f' and the outer single quotes
        return url_string[2:-1]  # Slice off "f'" and "'"
    # Case: f"""...""" (less common but possible)
    elif url_string.startswith('f"""') and url_string.endswith('"""'):
        return url_string[4:-3]  # Slice off 'f"""' and '"""'
    # Case: f'''...''' (less common but possible)
    elif url_string.startswith("f'''") and url_string.endswith("'''"):
        return url_string[4:-3]  # Slice off "f'''" and "'''"

    # --- Handle standard string literal representations (after f-string checks) ---
    # Case: "..."
    if url_string.startswith('"') and url_string.endswith('"'):
        return url_string[1:-1]
    # Case: '...'
    elif url_string.startswith("'") and url_string.endswith("'"):
        return url_string[1:-1]
    # Case: """..."""
    elif url_string.startswith('"""') and url_string.endswith('"""'):
        return url_string[3:-3]
    # Case: '''...'''
    elif url_string.startswith("'''") and url_string.endswith("'''"):
        return url_string[3:-3]

    # If no matching quotes or f-string patterns are found, return the original string
    return url_string


def map_tuple_arg(argval: tuple, original_argrepr: str) -> str:
    result = set()
    for item in argval:
        if isinstance(item, str):
            result.add(str(item))
        elif isinstance(item, int):
            result.add(SpecialCases.INTEGER.value)
        elif isinstance(item, float):
            result.add(SpecialCases.FLOAT.value)
    if not result:
        return ""
    ordered = list(result)
    ordered.sort()
    return " ".join(ordered)


@lru_cache(maxsize=8192)
def _is_bash_code_cached(text: str) -> bool:
    """
    Fast bash code detection for high-volume processing.
    Uses syntactical element pre-filter before expensive regex.
    Trades accuracy for speed - optimized for millions of files.
    """
    if not text or len(text) < 2:
        return False

    # Check for bash syntactical elements
    if not any(syntax in text for syntax in _BASH_SYNTACTICAL_ELEMENTS):
        return False

    # Only run expensive regex if bash syntax found
    return bool(BASH_PATTERN.search(text))


def is_bash_code(text: str, threshold: int = 3) -> bool:
    """
    Fast function to determine if text is likely bash code.
    Simplified for scalable preprocessing of large datasets.
    """
    if not isinstance(text, str):
        return False
    return _is_bash_code_cached(text)


@lru_cache(maxsize=8192)
def _is_code_cached(text: str) -> bool:
    """
    Fast code detection for high-volume processing.
    Requires BOTH keywords and syntactical elements.
    Trades accuracy for speed - optimized for millions of files.
    """
    if not text or len(text) < 3:
        return False

    # Check for code syntactical elements
    if not any(syntax in text for syntax in _CODE_SYNTACTICAL_ELEMENTS):
        return False

    # Only run expensive regex if both syntax and keywords found
    return bool(CODE_PATTERN.search(text))


def is_code(text: str, threshold: float = 0.3) -> bool:
    """
    Fast function to determine if text likely contains code.
    Simplified for scalable preprocessing of large datasets.
    """
    if not isinstance(text, str):
        return False
    return _is_code_cached(text)


@lru_cache(maxsize=8192)
def _is_sql_cached(text: str) -> bool:
    """
    Fast SQL detection for high-volume processing.
    Uses SQL pattern pre-filter before expensive regex.
    Trades accuracy for speed - optimized for millions of files.
    """
    if not text or len(text) < 6:
        return False

    # Check for SQL structural patterns
    text_lower = text.lower()
    if not any(pattern.lower() in text_lower for pattern in _SQL_SYNTACTICAL_PATTERNS):
        return False

    # Only run expensive regex if SQL patterns found
    return bool(SQL_PATTERN.search(text))


def is_sql(text: str) -> bool:
    """
    Fast function to determine if text contains SQL statements.
    Simplified for scalable preprocessing of large datasets.
    """
    if not isinstance(text, str):
        return False
    return _is_sql_cached(text)
